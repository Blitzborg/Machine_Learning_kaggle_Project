{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import os\nimport cv2\nimport h5py\nimport zipfile\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport pandas as pd\nfrom collections import defaultdict\nfrom glob import glob\nfrom random import choice, sample\nfrom tqdm import tqdm\nfrom keras.callbacks import ModelCheckpoint, ReduceLROnPlateau, EarlyStopping\nfrom keras.layers import Input, Dense, GlobalMaxPool2D, GlobalAvgPool2D, Concatenate, Multiply, Dropout, Subtract\nfrom keras.models import Model\nfrom keras.optimizers import Adam\nfrom keras.preprocessing import image","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(os.listdir(\"../input\"))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"!pip install git+https://github.com/rcmalli/keras-vggface.git","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from keras_vggface.vggface import VGGFace\nfrom keras_vggface.utils import preprocess_input","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Will unzip the files so that you can see them..\nwith zipfile.ZipFile(\"../input/recognizing-faces-in-the-wild/train.zip\",\"r\") as z:\n    z.extractall(\"./train\")\nwith zipfile.ZipFile(\"../input/recognizing-faces-in-the-wild/test.zip\",\"r\") as z:\n    z.extractall(\"./test\")\n    \ntrain_relationships_file_path = \"../input/recognizing-faces-in-the-wild/train_relationships.csv\"\ntraining_data_folders_path = \"./train/\"\nval_families = \"F00\"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"all_images = glob(training_data_folders_path + \"*/*/*.jpg\")\n# print(all_images)  # list all the images\ntraining_data_images = [x for x in all_images]  # list all the images\nvalidation_set_images = [x for x in all_images if val_families in x] # list images that belong to val_families\n# print(validation_set_images)\ntrain_person_to_images_map = defaultdict(list)\n\nppl = [x.split(\"/\")[-3] + \"/\" + x.split(\"/\")[-2] for x in all_images]\n# print(ppl)\n# print(len(ppl))  # 12379 pics in total\n\nfor x in training_data_images:\n    train_person_to_images_map[x.split(\"/\")[-3] + \"/\" + x.split(\"/\")[-2]].append(x)  # total number of ppl in the data set\n    \n# print(train_person_to_images_map)  # segregates pics of each person in each family of train images\n# print(len(train_person_to_images_map))  # 2316 ppl in total\n\nval_person_to_images_map = defaultdict(list)\n\nfor x in validation_set_images:\n    val_person_to_images_map[x.split(\"/\")[-3] + \"/\" + x.split(\"/\")[-2]].append(x)\n\n# print(val_person_to_images_map)  # segregates pics of each person in each family of val images\n# print(len(val_person_to_images_map))  # 263 ppl in total","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"relationships = pd.read_csv(train_relationships_file_path)\n# print(relationships)\nrelationships = list(zip(relationships.p1.values, relationships.p2.values))\n# print(relationships)\n# print(len(relationships))  # 3598 realtions or rows in the csv file\nrelationships = [x for x in relationships if x[0] in ppl and x[1] in ppl]  # to eliminate the false relations\n# relationships = [x for x in relationships if x[0] in ppl and x[1] not in ppl]\n# print(relationships)\n# print(len(relationships))  # 3362 true relations\n\ntrain_relationships = [x for x in relationships if val_families not in x[0]]  # all the relations without the val_families\n# print(train_relationships)\nval_relationships = [x for x in relationships if val_families in x[0]]  # relations only with the val_families\n# print(val_relationships)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def read_img(path):  # read_img function with path parameter\n    img = image.load_img(path, target_size=(224, 224))  # loading the image as 224*224 size\n    img = np.array(img).astype(np.float)  # converting the image into a array of floats\n    return preprocess_input(img, version=2)  # normalize each pixel value\n\ndef gen(list_tuples, person_to_images_map, batch_size=16):\n    ppl = list(person_to_images_map.keys())\n    while True:\n        batch_tuples = sample(list_tuples, batch_size // 2)\n        labels = [1] * len(batch_tuples)\n        while len(batch_tuples) < batch_size:\n            p1 = choice(ppl)\n            p2 = choice(ppl)\n\n            if p1 != p2 and (p1, p2) not in list_tuples and (p2, p1) not in list_tuples:\n                batch_tuples.append((p1, p2))\n                labels.append(0)\n\n        for x in batch_tuples:\n            if not len(person_to_images_map[x[0]]):\n                print(x[0])\n\n        X1 = [choice(person_to_images_map[x[0]]) for x in batch_tuples]\n        X1 = np.array([read_img(x) for x in X1])\n\n        X2 = [choice(person_to_images_map[x[1]]) for x in batch_tuples]\n        X2 = np.array([read_img(x) for x in X2])\n\n        yield [X1, X2], labels\n\n\ndef baseline_model():\n    input_1 = Input(shape=(224, 224, 3))\n    input_2 = Input(shape=(224, 224, 3))\n\n    base_model = VGGFace(model='resnet50', include_top=False)  \n\n    for x in base_model.layers[:-3]:\n        x.trainable = True\n\n    x1 = base_model(input_1)\n    x2 = base_model(input_2)\n\n    x1 = Concatenate(axis=-1)([GlobalMaxPool2D()(x1), GlobalAvgPool2D()(x1)])  # next layer\n    x2 = Concatenate(axis=-1)([GlobalMaxPool2D()(x2), GlobalAvgPool2D()(x2)])\n\n    x3 = Subtract()([x1, x2])  # next layer\n    x3 = Multiply()([x3, x3])  # next layer\n\n    x1_ = Multiply()([x1, x1])\n    x2_ = Multiply()([x2, x2])\n    x4  = Subtract()([x1_, x2_])\n    \n    x5 = Multiply()([x1, x2])\n    \n    x = Concatenate(axis=-1)([x4, x3, x5])\n\n    x = Dense(100, activation=\"relu\")(x)\n    x = Dropout(0.01)(x)\n    out = Dense(1, activation=\"sigmoid\")(x)\n\n    model = Model([input_1, input_2], out)\n\n    model.compile(loss=\"binary_crossentropy\", metrics=['acc'], optimizer=Adam(0.00001))\n\n    model.summary()\n\n    return model","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"file_path = \"./vgg_face.h5\"\n\n#verbose is a parameter which deicdes how much information is to be displayed on the terminal every epoch \ncheckpoint = ModelCheckpoint(file_path, monitor='val_acc', verbose=1, save_best_only=True, mode='max')\n# A function to save the model after monitering a value ( val_acc ), once when it has reached \n#the best value (save_best_only) when the value is its max value ( mode = 'max')\n\nreduce_on_plateau = ReduceLROnPlateau(monitor=\"val_acc\", mode=\"max\", factor=0.1, patience=20, verbose=1)\n# A function to reduce the learning rate by  a factor (factor) based on monitering a value (val_acc) if it has no improvemnt \n#from its best score (mode = 'max') for a few epochs (patience)\n\nes = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=50)\n# A fucntion to stop the training after a few rounds (patience) if there is no improvement on the value being monitered\n#( loss ) based on its best score depending if it is to be maximised or minimised (mode)\n\ncallbacks_list = [checkpoint, reduce_on_plateau, es]\n\ncurr_model = baseline_model()  # initializing model with the given layes\n# curr_model.load_weights(file_path)\ncurr_model_hist = curr_model.fit_generator(gen(train_relationships, train_person_to_images_map, batch_size=16), use_multiprocessing=True,\n                    validation_data=gen(val_relationships, val_person_to_images_map, batch_size=16), epochs=120, verbose=2,\n                    workers=4, callbacks=callbacks_list, steps_per_epoch=200, validation_steps=10)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import matplotlib.pyplot as plt\ndef plot_accuracy(y):\n    if(y == True):\n        plt.plot(curr_model_hist.history['acc'])\n        plt.plot(curr_model_hist.history['val_acc'])\n        plt.legend(['train', 'validation'], loc='lower right')\n        plt.title('accuracy plot - train vs validation')\n        plt.xlabel('epoch')\n        plt.ylabel('accuracy')\n        plt.show()\n    else:\n        pass\n    return\n\ndef plot_loss(y):\n    if(y == True):\n        plt.plot(curr_model_hist.history['loss'])\n        plt.plot(curr_model_hist.history['val_loss'])\n        plt.legend(['training loss', 'validation loss'], loc = 'upper right')\n        plt.title('loss plot - training vs vaidation')\n        plt.xlabel('epoch')\n        plt.ylabel('loss')\n        plt.show()\n    else:\n        pass\n    return\n\n\nplot_accuracy(True)\nplot_loss(True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"b8da009689331982f628256abc151cbbd7e288a1"},"cell_type":"code","source":"# test_path = \"./test/\"\n\n# def chunker(seq, size=32):\n#     return (seq[pos:pos + size] for pos in range(0, len(seq), size))\n\n# submission = pd.read_csv( '../input/recognizing-faces-in-the-wild/sample_submission.csv')\n\n# predictions = []\n\n# for batch in tqdm(chunker(submission.img_pair.values)):\n#     X1 = [x.split(\"-\")[0] for x in batch]\n#     X1 = np.array([read_img(test_path + x) for x in X1])\n\n#     X2 = [x.split(\"-\")[1] for x in batch]\n#     X2 = np.array([read_img(test_path + x) for x in X2])\n    \n#     prediction = 0\n#     for i in ['00','01','02','03','04','05','06','07','08','09']:\n#         curr_model.load_weights('../input/valaccall/Val_acc_f'+ i +'.h5')\n#         prediction = prediction + curr_model.predict([X1, X2])\n#     for i in ['00','01','02','03','04','05','06','07','08','09']:\n#         curr_model.load_weights('../input/vjmodels/F'+ i +'_val_loss.h5')\n#         prediction = prediction + curr_model.predict([X1, X2])\n#     prediction = prediction/20\n#     pred = prediction.ravel().tolist()\n#     predictions += pred\n\n# submission['is_related'] = predictions\n\n# submission.to_csv(\"CSVFinal.csv\", index=False)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":4}