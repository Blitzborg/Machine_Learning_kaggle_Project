{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import os\nimport cv2\nimport h5py\nimport zipfile\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport pandas as pd\nfrom collections import defaultdict\nfrom glob import glob\nfrom random import choice, sample\nfrom tqdm import tqdm\nfrom keras.callbacks import ModelCheckpoint, ReduceLROnPlateau, EarlyStopping\nfrom keras.layers import Input, Dense, GlobalMaxPool2D, GlobalAvgPool2D, Concatenate, Multiply, Dropout, Subtract\nfrom keras.models import Model\nfrom keras.optimizers import Adam\nfrom keras.preprocessing import image","execution_count":1,"outputs":[{"output_type":"stream","text":"Using TensorFlow backend.\n","name":"stderr"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(os.listdir(\"../input\"))","execution_count":2,"outputs":[{"output_type":"stream","text":"['vjmodels', 'valaccall', 'recognizing-faces-in-the-wild']\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"!pip install git+https://github.com/rcmalli/keras-vggface.git","execution_count":3,"outputs":[{"output_type":"stream","text":"Collecting git+https://github.com/rcmalli/keras-vggface.git\n  Cloning https://github.com/rcmalli/keras-vggface.git to /tmp/pip-req-build-tprbwnr2\n  Running command git clone -q https://github.com/rcmalli/keras-vggface.git /tmp/pip-req-build-tprbwnr2\nRequirement already satisfied: numpy>=1.9.1 in /opt/conda/lib/python3.6/site-packages (from keras-vggface==0.6) (1.16.4)\nRequirement already satisfied: scipy>=0.14 in /opt/conda/lib/python3.6/site-packages (from keras-vggface==0.6) (1.2.1)\nRequirement already satisfied: h5py in /opt/conda/lib/python3.6/site-packages (from keras-vggface==0.6) (2.9.0)\nRequirement already satisfied: pillow in /opt/conda/lib/python3.6/site-packages (from keras-vggface==0.6) (5.4.1)\nRequirement already satisfied: keras in /opt/conda/lib/python3.6/site-packages (from keras-vggface==0.6) (2.2.4)\nRequirement already satisfied: six>=1.9.0 in /opt/conda/lib/python3.6/site-packages (from keras-vggface==0.6) (1.12.0)\nRequirement already satisfied: pyyaml in /opt/conda/lib/python3.6/site-packages (from keras-vggface==0.6) (5.1.1)\nRequirement already satisfied: keras-applications>=1.0.6 in /opt/conda/lib/python3.6/site-packages (from keras->keras-vggface==0.6) (1.0.8)\nRequirement already satisfied: keras-preprocessing>=1.0.5 in /opt/conda/lib/python3.6/site-packages (from keras->keras-vggface==0.6) (1.1.0)\nBuilding wheels for collected packages: keras-vggface\n  Building wheel for keras-vggface (setup.py) ... \u001b[?25ldone\n\u001b[?25h  Created wheel for keras-vggface: filename=keras_vggface-0.6-cp36-none-any.whl size=8311 sha256=8e8c98d02611e6cf17e901d3b8ca416cfb3d0c74073e0e01cc55516dd4240af1\n  Stored in directory: /tmp/pip-ephem-wheel-cache-fq46walh/wheels/36/07/46/06c25ce8e9cd396dabe151ea1d8a2bc28dafcb11321c1f3a6d\nSuccessfully built keras-vggface\nInstalling collected packages: keras-vggface\nSuccessfully installed keras-vggface-0.6\n\u001b[33mWARNING: You are using pip version 19.2.1, however version 20.1 is available.\nYou should consider upgrading via the 'pip install --upgrade pip' command.\u001b[0m\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"from keras_vggface.vggface import VGGFace\nfrom keras_vggface.utils import preprocess_input","execution_count":4,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Will unzip the files so that you can see them..\nwith zipfile.ZipFile(\"../input/recognizing-faces-in-the-wild/train.zip\",\"r\") as z:\n    z.extractall(\"./train\")\nwith zipfile.ZipFile(\"../input/recognizing-faces-in-the-wild/test.zip\",\"r\") as z:\n    z.extractall(\"./test\")\n    \ntrain_relationships_file_path = \"../input/recognizing-faces-in-the-wild/train_relationships.csv\"\ntraining_data_folders_path = \"./train/\"\nval_families = \"F00\"","execution_count":5,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"all_images = glob(training_data_folders_path + \"*/*/*.jpg\")\n# print(all_images)  # list all the images\ntraining_data_images = [x for x in all_images]  # list all the images\nvalidation_set_images = [x for x in all_images if val_families in x] # list images that belong to val_families\n# print(validation_set_images)\ntrain_person_to_images_map = defaultdict(list)\n\nppl = [x.split(\"/\")[-3] + \"/\" + x.split(\"/\")[-2] for x in all_images]\n# print(ppl)\n# print(len(ppl))  # 12379 pics in total\n\nfor x in training_data_images:\n    train_person_to_images_map[x.split(\"/\")[-3] + \"/\" + x.split(\"/\")[-2]].append(x)  # total number of ppl in the data set\n    \n# print(train_person_to_images_map)  # segregates pics of each person in each family of train images\n# print(len(train_person_to_images_map))  # 2316 ppl in total\n\nval_person_to_images_map = defaultdict(list)\n\nfor x in validation_set_images:\n    val_person_to_images_map[x.split(\"/\")[-3] + \"/\" + x.split(\"/\")[-2]].append(x)\n\n# print(val_person_to_images_map)  # segregates pics of each person in each family of val images\n# print(len(val_person_to_images_map))  # 263 ppl in total","execution_count":6,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"relationships = pd.read_csv(train_relationships_file_path)\n# print(relationships)\nrelationships = list(zip(relationships.p1.values, relationships.p2.values))\n# print(relationships)\n# print(len(relationships))  # 3598 realtions or rows in the csv file\nrelationships = [x for x in relationships if x[0] in ppl and x[1] in ppl]  # to eliminate the false relations\n# relationships = [x for x in relationships if x[0] in ppl and x[1] not in ppl]\n# print(relationships)\n# print(len(relationships))  # 3362 true relations\n\ntrain_relationships = [x for x in relationships if val_families not in x[0]]  # all the relations without the val_families\n# print(train_relationships)\nval_relationships = [x for x in relationships if val_families in x[0]]  # relations only with the val_families\n# print(val_relationships)","execution_count":7,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def read_img(path):  # read_img function with path parameter\n    img = image.load_img(path, target_size=(224, 224))  # loading the image as 224*224 size\n    img = np.array(img).astype(np.float)  # converting the image into a array of floats\n    return preprocess_input(img, version=2)  # normalize each pixel value\n\ndef gen(list_tuples, person_to_images_map, batch_size=16):\n    ppl = list(person_to_images_map.keys())\n    while True:\n        batch_tuples = sample(list_tuples, batch_size // 2)\n        labels = [1] * len(batch_tuples)\n        while len(batch_tuples) < batch_size:\n            p1 = choice(ppl)\n            p2 = choice(ppl)\n\n            if p1 != p2 and (p1, p2) not in list_tuples and (p2, p1) not in list_tuples:\n                batch_tuples.append((p1, p2))\n                labels.append(0)\n\n        for x in batch_tuples:\n            if not len(person_to_images_map[x[0]]):\n                print(x[0])\n\n        X1 = [choice(person_to_images_map[x[0]]) for x in batch_tuples]\n        X1 = np.array([read_img(x) for x in X1])\n\n        X2 = [choice(person_to_images_map[x[1]]) for x in batch_tuples]\n        X2 = np.array([read_img(x) for x in X2])\n\n        yield [X1, X2], labels\n\n\ndef baseline_model():\n    input_1 = Input(shape=(224, 224, 3))\n    input_2 = Input(shape=(224, 224, 3))\n\n    base_model = VGGFace(model='resnet50', include_top=False)  \n\n    for x in base_model.layers[:-3]:\n        x.trainable = True\n\n    x1 = base_model(input_1)\n    x2 = base_model(input_2)\n\n    x1 = Concatenate(axis=-1)([GlobalMaxPool2D()(x1), GlobalAvgPool2D()(x1)])  # next layer\n    x2 = Concatenate(axis=-1)([GlobalMaxPool2D()(x2), GlobalAvgPool2D()(x2)])\n\n    x3 = Subtract()([x1, x2])  # next layer\n    x3 = Multiply()([x3, x3])  # next layer\n\n    x1_ = Multiply()([x1, x1])\n    x2_ = Multiply()([x2, x2])\n    x4  = Subtract()([x1_, x2_])\n    \n    x5 = Multiply()([x1, x2])\n    \n    x = Concatenate(axis=-1)([x4, x3, x5])\n\n    x = Dense(100, activation=\"relu\")(x)\n    x = Dropout(0.01)(x)\n    out = Dense(1, activation=\"sigmoid\")(x)\n\n    model = Model([input_1, input_2], out)\n\n    model.compile(loss=\"binary_crossentropy\", metrics=['acc'], optimizer=Adam(0.00001))\n\n    model.summary()\n\n    return model","execution_count":8,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"file_path = \"./vgg_face.h5\"\n\n#verbose is a parameter which deicdes how much information is to be displayed on the terminal every epoch \ncheckpoint = ModelCheckpoint(file_path, monitor='val_acc', verbose=1, save_best_only=True, mode='max')\n# A function to save the model after monitering a value ( val_acc ), once when it has reached \n#the best value (save_best_only) when the value is its max value ( mode = 'max')\n\nreduce_on_plateau = ReduceLROnPlateau(monitor=\"val_acc\", mode=\"max\", factor=0.1, patience=20, verbose=1)\n# A function to reduce the learning rate by  a factor (factor) based on monitering a value (val_acc) if it has no improvemnt \n#from its best score (mode = 'max') for a few epochs (patience)\n\nes = EarlyStopping(monitor='loss', mode='min', verbose=1, patience=50)\n# A fucntion to stop the training after a few rounds (patience) if there is no improvement on the value being monitered\n#( loss ) based on its best score depending if it is to be maximised or minimised (mode)\n\ncallbacks_list = [checkpoint, reduce_on_plateau, es]\n\ncurr_model = baseline_model()  # initializing model with the given layes\n# curr_model.load_weights(file_path)\ncurr_model_hist = curr_model.fit_generator(gen(train_relationships, train_person_to_images_map, batch_size=16), use_multiprocessing=True,\n                    validation_data=gen(val_relationships, val_person_to_images_map, batch_size=16), epochs=2, verbose=2,\n                    workers=4, callbacks=callbacks_list, steps_per_epoch=200, validation_steps=10)","execution_count":null,"outputs":[{"output_type":"stream","text":"Downloading data from https://github.com/rcmalli/keras-vggface/releases/download/v2.0/rcmalli_vggface_tf_notop_resnet50.h5\n94699520/94694792 [==============================] - 3s 0us/step\n__________________________________________________________________________________________________\nLayer (type)                    Output Shape         Param #     Connected to                     \n==================================================================================================\ninput_1 (InputLayer)            (None, 224, 224, 3)  0                                            \n__________________________________________________________________________________________________\ninput_2 (InputLayer)            (None, 224, 224, 3)  0                                            \n__________________________________________________________________________________________________\nvggface_resnet50 (Model)        multiple             23561152    input_1[0][0]                    \n                                                                 input_2[0][0]                    \n__________________________________________________________________________________________________\nglobal_max_pooling2d_1 (GlobalM (None, 2048)         0           vggface_resnet50[1][0]           \n__________________________________________________________________________________________________\nglobal_average_pooling2d_1 (Glo (None, 2048)         0           vggface_resnet50[1][0]           \n__________________________________________________________________________________________________\nglobal_max_pooling2d_2 (GlobalM (None, 2048)         0           vggface_resnet50[2][0]           \n__________________________________________________________________________________________________\nglobal_average_pooling2d_2 (Glo (None, 2048)         0           vggface_resnet50[2][0]           \n__________________________________________________________________________________________________\nconcatenate_1 (Concatenate)     (None, 4096)         0           global_max_pooling2d_1[0][0]     \n                                                                 global_average_pooling2d_1[0][0] \n__________________________________________________________________________________________________\nconcatenate_2 (Concatenate)     (None, 4096)         0           global_max_pooling2d_2[0][0]     \n                                                                 global_average_pooling2d_2[0][0] \n__________________________________________________________________________________________________\nmultiply_2 (Multiply)           (None, 4096)         0           concatenate_1[0][0]              \n                                                                 concatenate_1[0][0]              \n__________________________________________________________________________________________________\nmultiply_3 (Multiply)           (None, 4096)         0           concatenate_2[0][0]              \n                                                                 concatenate_2[0][0]              \n__________________________________________________________________________________________________\nsubtract_1 (Subtract)           (None, 4096)         0           concatenate_1[0][0]              \n                                                                 concatenate_2[0][0]              \n__________________________________________________________________________________________________\nsubtract_2 (Subtract)           (None, 4096)         0           multiply_2[0][0]                 \n                                                                 multiply_3[0][0]                 \n__________________________________________________________________________________________________\nmultiply_1 (Multiply)           (None, 4096)         0           subtract_1[0][0]                 \n                                                                 subtract_1[0][0]                 \n__________________________________________________________________________________________________\nmultiply_4 (Multiply)           (None, 4096)         0           concatenate_1[0][0]              \n                                                                 concatenate_2[0][0]              \n__________________________________________________________________________________________________\nconcatenate_3 (Concatenate)     (None, 12288)        0           subtract_2[0][0]                 \n                                                                 multiply_1[0][0]                 \n                                                                 multiply_4[0][0]                 \n__________________________________________________________________________________________________\ndense_1 (Dense)                 (None, 100)          1228900     concatenate_3[0][0]              \n__________________________________________________________________________________________________\ndropout_1 (Dropout)             (None, 100)          0           dense_1[0][0]                    \n__________________________________________________________________________________________________\ndense_2 (Dense)                 (None, 1)            101         dropout_1[0][0]                  \n==================================================================================================\nTotal params: 24,790,153\nTrainable params: 24,737,033\nNon-trainable params: 53,120\n__________________________________________________________________________________________________\nEpoch 1/2\n","name":"stdout"},{"output_type":"stream","text":"/opt/conda/lib/python3.6/site-packages/keras/engine/training_generator.py:47: UserWarning: Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the`keras.utils.Sequence class.\n  UserWarning('Using a generator with `use_multiprocessing=True`'\n","name":"stderr"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"import matplotlib.pyplot as plt\ndef plot_accuracy(y):\n    if(y == True):\n        plt.plot(curr_model_hist.history['acc'])\n        plt.plot(curr_model_hist.history['val_acc'])\n        plt.legend(['train', 'validation'], loc='lower right')\n        plt.title('accuracy plot - train vs validation')\n        plt.xlabel('epoch')\n        plt.ylabel('accuracy')\n        plt.show()\n    else:\n        pass\n    return\n\ndef plot_loss(y):\n    if(y == True):\n        plt.plot(curr_model_hist.history['loss'])\n        plt.plot(curr_model_hist.history['val_loss'])\n        plt.legend(['training loss', 'validation loss'], loc = 'upper right')\n        plt.title('loss plot - training vs vaidation')\n        plt.xlabel('epoch')\n        plt.ylabel('loss')\n        plt.show()\n    else:\n        pass\n    return\n\n\nplot_accuracy(True)\nplot_loss(True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"b8da009689331982f628256abc151cbbd7e288a1"},"cell_type":"code","source":"test_path = \"./test/\"\n\ndef chunker(seq, size=32):\n    return (seq[pos:pos + size] for pos in range(0, len(seq), size))\n\nfrom tqdm import tqdm\n\nsubmission = pd.read_csv( '../input/recognizing-faces-in-the-wild/sample_submission.csv')\n\npredictions = []\n\nfor batch in tqdm(chunker(submission.img_pair.values)):\n    X1 = [x.split(\"-\")[0] for x in batch]\n    X1 = np.array([read_img(test_path + x) for x in X1])\n\n    X2 = [x.split(\"-\")[1] for x in batch]\n    X2 = np.array([read_img(test_path + x) for x in X2])\n    \n    pred = curr_model.predict([X1, X2]).ravel().tolist()  # input X1, X2 to get the output value\n    predictions += pred\n\n#   code for ensembling\n#     prediction = 0\n#     for i in ['00','01','02','03','04','05','06','07','08','08_2','09']:\n#         curr_model.load_weights('../input/valaccall/Val_acc_f'+ i +'.h5')\n#         prediction = prediction + curr_model.predict([X1, X2])\n#     for i in ['00','01','02','03','04','05','06','07','08','09']:\n#         curr_model.load_weights('../input/vjmodels/F'+ i +'_val_loss.h5')\n#         prediction = prediction + curr_model.predict([X1, X2])\n#     prediction = prediction/21\n#     pred = prediction.ravel().tolist()\n#     predictions += pred\n\nsubmission['is_related'] = predictions\n\nsubmission.to_csv(\"CSVFinal1.csv\", index=False)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":4}